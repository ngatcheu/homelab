# D√©ploiement Kubernetes sur Proxmox avec Terraform

Infrastructure as Code pour d√©ployer un cluster Kubernetes sur Proxmox VE avec 10 VMs :
- 3 VMs Rancher (Control Plane)
- 3 VMs Payload Masters
- 3 VMs Payload Workers
- 1 VM CI/CD

## D√©marrage rapide

**Nouveau ?** Suivez le guide complet : [PHASE1-DEPLOYMENT.md](PHASE1-DEPLOYMENT.md)

**Guide pas √† pas Phase 1** :
1. V√©rifier pr√©requis ‚Üí [check-proxmox-prereqs.sh](check-proxmox-prereqs.sh)
2. Cr√©er template Rocky 9 ‚Üí [create-rocky9-template.sh](create-rocky9-template.sh)
3. Cr√©er bridges r√©seau ‚Üí [create-network-bridges.sh](create-network-bridges.sh)
4. D√©ployer avec Terraform ‚Üí `terraform apply`

**Voir aussi** : [ROADMAP-DEVSECOPS.md](../ROADMAP-DEVSECOPS.md) pour la vue d'ensemble compl√®te

## Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Proxmox VE Cluster                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  üéØ RANCHER (Control Plane)                                  ‚îÇ
‚îÇ  ‚îú‚îÄ rancher-1      ‚Üí 110 ‚Üí 192.168.1.110 ‚Üí 2C/8GB           ‚îÇ
‚îÇ  ‚îú‚îÄ rancher-2      ‚Üí 111 ‚Üí 192.168.1.111 ‚Üí 2C/8GB           ‚îÇ
‚îÇ  ‚îî‚îÄ rancher-3      ‚Üí 112 ‚Üí 192.168.1.112 ‚Üí 2C/8GB           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  üîß PAYLOAD MASTERS                                          ‚îÇ
‚îÇ  ‚îú‚îÄ payload-master-1 ‚Üí 113 ‚Üí 192.168.1.113 ‚Üí 2C/4GB         ‚îÇ
‚îÇ  ‚îú‚îÄ payload-master-2 ‚Üí 114 ‚Üí 192.168.1.114 ‚Üí 2C/4GB         ‚îÇ
‚îÇ  ‚îî‚îÄ payload-master-3 ‚Üí 115 ‚Üí 192.168.1.115 ‚Üí 2C/4GB         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚öôÔ∏è  PAYLOAD WORKERS                                         ‚îÇ
‚îÇ  ‚îú‚îÄ payload-worker-1 ‚Üí 116 ‚Üí 192.168.1.116 ‚Üí 3C/8GB         ‚îÇ
‚îÇ  ‚îú‚îÄ payload-worker-2 ‚Üí 117 ‚Üí 192.168.1.117 ‚Üí 3C/8GB         ‚îÇ
‚îÇ  ‚îî‚îÄ payload-worker-3 ‚Üí 118 ‚Üí 192.168.1.118 ‚Üí 3C/8GB         ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  üì¶ SERVICES                                                 ‚îÇ
‚îÇ  ‚îî‚îÄ cicd           ‚Üí 119 ‚Üí 192.168.1.119 ‚Üí 2C/8GB           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Pr√©requis

### Logiciels requis
- Terraform >= 1.12.2
- Acc√®s √† un serveur Proxmox VE
- Connexion r√©seau au serveur Proxmox

### Infrastructure Proxmox
- Serveur Proxmox VE op√©rationnel
- Stockage `local-lvm` disponible
- Bridge r√©seau `vmbr0` configur√©
- Acc√®s root au serveur Proxmox

## Installation

### √âtape 1 : Cr√©er le template Rocky Linux 9

Avant de d√©ployer les VMs, vous devez cr√©er un template cloud-init Rocky Linux 9 sur votre serveur Proxmox.

**Option A : Via le script fourni**

Copiez et ex√©cutez le script sur votre serveur Proxmox :

```bash
# Copier le script sur Proxmox
scp create-rocky9-template.sh root@192.168.1.100:/tmp/

# Se connecter √† Proxmox
ssh root@192.168.1.100

# Ex√©cuter le script
chmod +x /tmp/create-rocky9-template.sh
/tmp/create-rocky9-template.sh
```

**Option B : Via le Shell Proxmox Web UI**

1. Connectez-vous √† l'interface web Proxmox : https://192.168.1.100:8006
2. Cliquez sur votre node ‚Üí Shell
3. Copiez-collez le contenu du script [create-rocky9-template.sh](create-rocky9-template.sh)

Le template cr√©√© aura :
- **ID** : 9100
- **Nom** : rocky-9-cloud-template
- **Stockage** : local-lvm

### √âtape 2 : Configurer Terraform

Configurez vos variables dans le fichier `terraform.tfvars` :

```hcl
# Mot de passe Proxmox (OBLIGATOIRE)
proxmox_password = "votre-mot-de-passe-root"

# Cl√© SSH publique pour l'acc√®s aux VMs (OBLIGATOIRE)
ssh_public_key = "ssh-rsa AAAAB3... votre-cl√©-publique"

# Les autres valeurs ont des valeurs par d√©faut
```

### √âtape 3 : Initialiser Terraform

```bash
# Initialiser Terraform et t√©l√©charger le provider
terraform init

# V√©rifier la configuration
terraform validate

# Voir le plan de d√©ploiement
terraform plan
```

### √âtape 4 : D√©ployer les VMs

```bash
# D√©ployer l'infrastructure
terraform apply

# Confirmer avec 'yes' quand demand√©
```

Le d√©ploiement prend environ 5-10 minutes.

## Configuration

### Variables disponibles

#### Configuration Proxmox
```hcl
proxmox_node     = "devsecops-dojo"  # Nom du node Proxmox
proxmox_host     = "192.168.1.100"   # IP du serveur Proxmox
proxmox_password = ""                # Mot de passe root (OBLIGATOIRE)
```

#### Configuration VMs
```hcl
vm_id_start = 102                    # Premier ID de VM

# Rancher (Control Plane)
rancher_cpu_cores = 2
rancher_memory    = 8192             # RAM en MB

# Payload Masters
payload_master_cpu_cores = 2
payload_master_memory    = 4096

# Payload Workers
payload_worker_cpu_cores = 3
payload_worker_memory    = 8192

# CI/CD
cicd_cpu_cores = 2
cicd_memory    = 8192
```

#### Configuration R√©seau
```hcl
network_bridge   = "vmbr0"           # Bridge r√©seau
ip_address_base  = "192.168.1"       # Base des IPs
ip_start         = 110               # Premi√®re IP : 192.168.1.110
gateway          = "192.168.1.1"     # Passerelle
nameserver       = "192.168.1.1"     # Serveur DNS
```

#### Cl√© SSH
```hcl
ssh_public_key = "ssh-rsa AAAAB3..."  # Votre cl√© SSH publique
```

## Gestion de l'infrastructure

### Voir les outputs

Apr√®s le d√©ploiement :

```bash
terraform output
```

Outputs disponibles :
- `rancher_vm_names` : Noms des VMs Rancher
- `rancher_vm_ids` : IDs des VMs Rancher
- `payload_vm_names` : Noms des VMs Payload
- `payload_vm_ids` : IDs des VMs Payload
- `deployment_summary` : R√©sum√© complet du d√©ploiement

### D√©marrer les VMs

Les VMs Kubernetes et CI/CD sont cr√©√©es mais non d√©marr√©es. Pour les d√©marrer :

```bash
# Depuis le serveur Proxmox - VMs Kubernetes + CI/CD
for i in {102..111}; do qm start $i; done

# OPNsense (VM 200) - D√©marrer manuellement apr√®s installation
qm start 200
```

### Se connecter aux VMs

```bash
# Se connecter √† la premi√®re VM Rancher
ssh root@192.168.1.110

# Ou utiliser l'IP de n'importe quelle VM
ssh root@192.168.1.113  # payload-master-1
```

### Modifier l'infrastructure

1. Modifiez les fichiers `.tf` ou `terraform.tfvars`
2. Planifiez les changements : `terraform plan`
3. Appliquez : `terraform apply`

### D√©truire l'infrastructure

```bash
# Supprimer toutes les VMs cr√©√©es
terraform destroy
```

## Scripts utilitaires

### cleanup-vms.sh

Nettoie les VMs existantes (102-110) en cas de conflit :

```bash
# Sur le serveur Proxmox
bash cleanup-vms.sh
```

Utilisez ce script si vous obtenez l'erreur "config file already exists".

## Configuration OPNsense Firewall

### Vue d'ensemble

La VM OPNsense fonctionne comme un **firewall interne** avec 3 interfaces r√©seau pour segmenter votre infrastructure :

- **VM ID** : 200
- **CPU** : 2 cores
- **RAM** : 2 GB
- **Disque** : 20 GB
- **Boot** : ISO OPNsense-25.7-dvd-amd64.iso

### Architecture R√©seau

**IMPORTANT : OPNsense est un firewall INTERNE, pas votre routeur principal !**

```
Internet ‚Üí Box (192.168.1.1) ‚Üí vmbr0 (192.168.1.0/24)
                                   ‚Üì                  ‚Üì
                              VMs existantes    OPNsense (192.168.1.200)
                           (Rancher, Payload)           ‚Üì
                             continuent avec        ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
                             box comme gateway      ‚Üì       ‚Üì
                                              vmbr1 (LAN) vmbr2 (OPT1)
                                           192.168.10.0/24 192.168.20.0/24
                                            Management    Production

Interfaces OPNsense:
  ‚Ä¢ vtnet0 (LAN)      ‚Üí vmbr1 ‚Üí 192.168.10.1/24  (Gateway Management)
  ‚Ä¢ vtnet1 (OPT1)     ‚Üí vmbr2 ‚Üí 192.168.20.1/24  (Gateway Production)
  ‚Ä¢ vtnet2 (OPT2)     ‚Üí vmbr0 ‚Üí 192.168.1.200/24 (Uplink Internet)
```

### Installation OPNsense

**1. D√©marrer la VM depuis Proxmox UI**

La VM est cr√©√©e en mode `started = false`. D√©marrez-la manuellement depuis l'interface Proxmox.

**2. Installation depuis le DVD**

```bash
# Login installateur
login: installer
password: opnsense

# Suivre l'assistant d'installation
# - Keymap : fr.kbd (ou us)
# - Install : Option 2 - Install (UFS)
# - Disk : da0
# - Root password : [votre mot de passe s√©curis√©]
# - Complete Install ‚Üí Reboot
```

**3. Assignment des interfaces (Premier boot)**

**IMPORTANT : OPNsense n'a PAS d'interface WAN dans cette configuration !**

```
Valid interfaces are:
vtnet0   BC:24:11:00:01:01  (vmbr1 - Management)
vtnet1   BC:24:11:00:01:02  (vmbr2 - Production)
vtnet2   BC:24:11:00:01:00  (vmbr0 - Uplink Internet)

Do you want to configure VLANs now? [y/n]: n

Enter the WAN interface name: [laissez VIDE - Appuyez sur Entr√©e]
Enter the LAN interface name: vtnet0
Enter the Optional 1 interface name: vtnet1
Enter the Optional 2 interface name: vtnet2
```

**Pourquoi pas de WAN ?** OPNsense est un firewall interne. L'acc√®s Internet passe par vtnet2 (OPT2) qui se connecte √† vmbr0 o√π votre box (192.168.1.1) reste le routeur principal.

**4. Configuration des interfaces (Console OPNsense)**

Menu principal ‚Üí Option 2: Set interface IP address

**Interface LAN (vtnet0 - vmbr1) :**
```
IPv4 address: 192.168.10.1
Subnet: 24
DHCP server: y
DHCP range: 192.168.10.100 - 192.168.10.200
```

**Interface OPT1 (vtnet1 - vmbr2) :**
```
IPv4 address: 192.168.20.1
Subnet: 24
DHCP server: y
DHCP range: 192.168.20.100 - 192.168.20.200
```

**Interface OPT2 (vtnet2 - vmbr0 Uplink) :**
```
IPv4 address: 192.168.1.200
Subnet: 24
DHCP server: n (PAS de DHCP sur l'uplink !)
```

**5. Configuration Web UI**

Acc√®s : https://192.168.10.1 (depuis une VM sur vmbr1)

```
Login: root
Password: [votre mot de passe]
```

**Configuration Gateway :**
- System ‚Üí Gateways ‚Üí Single
- Ajouter : BOX_GW ‚Üí Interface OPT2 ‚Üí IP 192.168.1.1
- Marquer comme default gateway

**R√®gles Firewall :**
- Firewall ‚Üí Rules ‚Üí LAN : Allow LAN to any
- Firewall ‚Üí Rules ‚Üí OPT1 : Allow OPT1 to any
- Firewall ‚Üí NAT ‚Üí Outbound : Mode Automatic

### R√©sum√© des Interfaces

| Interface | Bridge | IP OPNsense | R√©seau | DHCP Range | R√¥le | Config OPNsense |
|-----------|--------|-------------|---------|------------|------|-----------------|
| vtnet0 | vmbr1 | 192.168.10.1/24 | Management | .100-.200 | Gateway interne | **LAN** |
| vtnet1 | vmbr2 | 192.168.20.1/24 | Production | .100-.200 | Gateway interne | **OPT1** |
| vtnet2 | vmbr0 | 192.168.1.200/24 | Internet | Aucun | Uplink Internet | **OPT2** |

### Points Importants

- OPNsense n'est **PAS** la gateway principale de votre r√©seau
- Vos VMs existantes sur vmbr0 (Rancher, Payload) continuent d'utiliser votre box comme gateway
- Seules les nouvelles VMs sur vmbr1 (Management) ou vmbr2 (Production) utiliseront OPNsense
- Pas de conflit DHCP car OPNsense ne fait pas de DHCP sur vmbr0

## Prochaines √©tapes

Une fois les VMs d√©ploy√©es et d√©marr√©es :

1. **Installer et configurer OPNsense** (voir section ci-dessus)

2. **Installer RKE2 sur les nodes Rancher**
   ```bash
   # Se connecter au premier node Rancher
   ssh root@192.168.1.110

   # Installer RKE2
   curl -sfL https://get.rke2.io | sh -
   systemctl enable rke2-server.service
   systemctl start rke2-server.service
   ```

3. **Joindre les autres nodes Rancher au cluster**

4. **Configurer les Payload nodes**
   - Installer RKE2 en mode agent
   - Joindre au cluster Rancher

5. **Configurer DNS et Certificats**
   - Installer cert-manager pour certificats automatiques
   - D√©ployer Pi-hole pour DNS interne
   - Configurer CoreDNS pour r√©solution .local
   - Voir d√©tails complets dans [ROADMAP-DEVSECOPS.md](ROADMAP-DEVSECOPS.md) (Jour 8)

6. **D√©ployer vos applications**

## Structure du projet

```
proxmox-terraform/
‚îú‚îÄ‚îÄ README.md                      # Ce fichier
‚îú‚îÄ‚îÄ .gitignore                     # Fichiers √† ignorer par git
‚îú‚îÄ‚îÄ providers.tf                   # Configuration du provider bpg/proxmox
‚îú‚îÄ‚îÄ variables.tf                   # D√©finition des variables
‚îú‚îÄ‚îÄ terraform.tfvars               # Valeurs des variables
‚îú‚îÄ‚îÄ main.tf                        # D√©finition des 9 VMs
‚îú‚îÄ‚îÄ outputs.tf                     # Outputs Terraform
‚îú‚îÄ‚îÄ create-rocky9-template.sh      # Script de cr√©ation du template
‚îî‚îÄ‚îÄ cleanup-vms.sh                 # Script de nettoyage des VMs
```

## D√©pannage

### Erreur : "template not found"

Le template Rocky Linux 9 n'existe pas dans Proxmox.

**Solution** : Ex√©cutez [create-rocky9-template.sh](create-rocky9-template.sh) sur votre serveur Proxmox.

### Erreur : "config file already exists"

Des VMs avec les IDs 102-111 ou 200 existent d√©j√† dans Proxmox.

**Solution** :
```bash
# Supprimer les VMs Kubernetes/CI/CD (102-111)
for i in {102..111}; do qm destroy $i; done

# Supprimer OPNsense (200)
qm destroy 200
```

### Erreur : "permission denied"

Les permissions root ne sont pas suffisantes.

**Solution** : V√©rifiez que vous utilisez bien `root@pam` comme username dans [providers.tf](providers.tf).

### Les VMs ne d√©marrent pas

**Solution** : V√©rifiez les logs Proxmox :
```bash
# Sur le serveur Proxmox
qm status 102
journalctl -xe
```

### Connexion SSH refus√©e

**Solution** :
1. V√©rifiez que la VM est bien d√©marr√©e : `qm status 102`
2. V√©rifiez que cloud-init a bien configur√© votre cl√© SSH
3. Acc√©dez √† la console Proxmox pour v√©rifier

## S√©curit√©

- Le fichier `terraform.tfvars` contient des informations sensibles (mot de passe Proxmox)
- Il est automatiquement exclu du d√©p√¥t git via `.gitignore`
- Ne committez JAMAIS ce fichier dans git
- Utilisez des secrets managers (Vault, etc.) en production

## Provider

Ce projet utilise le provider **bpg/proxmox** (v0.50.0) :
- Documentation : https://registry.terraform.io/providers/bpg/proxmox/latest/docs
- Plus moderne et stable que telmate/proxmox
- Meilleur support de cloud-init

## Licence

MIT
